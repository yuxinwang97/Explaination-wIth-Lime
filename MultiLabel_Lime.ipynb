{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5raYJMU8TTY3",
        "outputId": "c85d8519-0abd-4dcb-f105-f2ad6418ea92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse\n",
        "from scipy.sparse import csr_matrix\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ4PTUhsUDhY",
        "outputId": "b47dfaa4-51f8-40bb-e3e7-459544bceb78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting libpecos\n",
            "  Downloading libpecos-0.2.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.2 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from libpecos) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from libpecos) (1.10.0+cu111)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.7/dist-packages (from libpecos) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from libpecos) (1.21.5)\n",
            "Collecting transformers>=4.1.1\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 82.0 MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92,>=0.1.86\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 80.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->libpecos) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->libpecos) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.0->libpecos) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.1.1->libpecos) (3.6.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 82.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=4.1.1->libpecos) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.1.1->libpecos) (4.11.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.1.1->libpecos) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.1.1->libpecos) (4.63.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 90.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 86.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.1.1->libpecos) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.1.1->libpecos) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.1.1->libpecos) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.1.1->libpecos) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.1.1->libpecos) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.1.1->libpecos) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.1.1->libpecos) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.1.1->libpecos) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.1.1->libpecos) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, libpecos\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 libpecos-0.2.3 pyyaml-6.0 sacremoses-0.0.49 sentencepiece-0.1.96 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install libpecos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBJ3HctdT7VA",
        "outputId": "c5fff5f1-3320-42c0-974b-7a64c6105329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/CS260c/Amazon13k_Bow/AmazonCat-13K.bow.zip\n",
            "   creating: AmazonCat-13K.bow/\n",
            "  inflating: AmazonCat-13K.bow/test.txt  \n",
            "  inflating: AmazonCat-13K.bow/train.txt  \n",
            "  inflating: AmazonCat-13K.bow/Yf.txt  \n",
            "  inflating: AmazonCat-13K.bow/Xf.txt  \n",
            "Archive:  /content/drive/MyDrive/CS260c/Amazon13k_Raw/AmazonCat-13K.raw.zip\n",
            "   creating: AmazonCat-13K.raw/\n",
            "  inflating: AmazonCat-13K.raw/Yf.txt  \n",
            "  inflating: AmazonCat-13K.raw/trn.json.gz  \n",
            "  inflating: AmazonCat-13K.raw/tst.json.gz  \n",
            "Archive:  /content/drive/MyDrive/CS260c/260CDL_22Winter.zip\n",
            "   creating: 260CDL_22Winter/\n",
            "  inflating: 260CDL_22Winter/test_explainer.sh  \n",
            "  inflating: 260CDL_22Winter/.DS_Store  \n",
            "  inflating: __MACOSX/260CDL_22Winter/._.DS_Store  \n",
            "  inflating: 260CDL_22Winter/test_vectorizer.sh  \n",
            "  inflating: __MACOSX/260CDL_22Winter/._test_vectorizer.sh  \n",
            "  inflating: 260CDL_22Winter/toy_test_xlinear.py  \n",
            "  inflating: __MACOSX/260CDL_22Winter/._toy_test_xlinear.py  \n",
            "  inflating: 260CDL_22Winter/config.json  \n",
            "   creating: 260CDL_22Winter/model/\n",
            "   creating: 260CDL_22Winter/tfidf-model/\n",
            "  inflating: 260CDL_22Winter/readme.txt  \n",
            "  inflating: __MACOSX/260CDL_22Winter/._readme.txt  \n",
            "  inflating: 260CDL_22Winter/toy_interpret_xlinear.py  \n",
            "   creating: 260CDL_22Winter/.git/\n",
            "  inflating: 260CDL_22Winter/model/.DS_Store  \n",
            "  inflating: __MACOSX/260CDL_22Winter/model/._.DS_Store  \n",
            "  inflating: 260CDL_22Winter/model/param.json  \n",
            "   creating: 260CDL_22Winter/model/ranker/\n",
            "  inflating: 260CDL_22Winter/tfidf-model/config.json  \n",
            "  inflating: 260CDL_22Winter/tfidf-model/vectorizer.pkl  \n",
            "  inflating: 260CDL_22Winter/.git/config  \n",
            "   creating: 260CDL_22Winter/.git/objects/\n",
            "  inflating: 260CDL_22Winter/.git/HEAD  \n",
            "   creating: 260CDL_22Winter/.git/info/\n",
            "   creating: 260CDL_22Winter/.git/logs/\n",
            "  inflating: 260CDL_22Winter/.git/description  \n",
            "   creating: 260CDL_22Winter/.git/hooks/\n",
            "   creating: 260CDL_22Winter/.git/refs/\n",
            "  inflating: 260CDL_22Winter/.git/index  \n",
            "  inflating: 260CDL_22Winter/.git/COMMIT_EDITMSG  \n",
            "  inflating: 260CDL_22Winter/model/ranker/.DS_Store  \n",
            "  inflating: __MACOSX/260CDL_22Winter/model/ranker/._.DS_Store  \n",
            "   creating: 260CDL_22Winter/model/ranker/2.model/\n",
            "  inflating: 260CDL_22Winter/model/ranker/param.json  \n",
            "   creating: 260CDL_22Winter/model/ranker/0.model/\n",
            "   creating: 260CDL_22Winter/model/ranker/1.model/\n",
            "   creating: 260CDL_22Winter/.git/objects/95/\n",
            "   creating: 260CDL_22Winter/.git/objects/57/\n",
            "   creating: 260CDL_22Winter/.git/objects/9b/\n",
            "   creating: 260CDL_22Winter/.git/objects/d9/\n",
            "   creating: 260CDL_22Winter/.git/objects/ac/\n",
            "   creating: 260CDL_22Winter/.git/objects/a5/\n",
            "   creating: 260CDL_22Winter/.git/objects/ab/\n",
            "   creating: 260CDL_22Winter/.git/objects/c1/\n",
            "   creating: 260CDL_22Winter/.git/objects/pack/\n",
            "   creating: 260CDL_22Winter/.git/objects/44/\n",
            "   creating: 260CDL_22Winter/.git/objects/6b/\n",
            "   creating: 260CDL_22Winter/.git/objects/6e/\n",
            "   creating: 260CDL_22Winter/.git/objects/09/\n",
            "   creating: 260CDL_22Winter/.git/objects/info/\n",
            "   creating: 260CDL_22Winter/.git/objects/6c/\n",
            "   creating: 260CDL_22Winter/.git/objects/ba/\n",
            "   creating: 260CDL_22Winter/.git/objects/a0/\n",
            "   creating: 260CDL_22Winter/.git/objects/c4/\n",
            "   creating: 260CDL_22Winter/.git/objects/cb/\n",
            "   creating: 260CDL_22Winter/.git/objects/46/\n",
            "   creating: 260CDL_22Winter/.git/objects/2c/\n",
            "   creating: 260CDL_22Winter/.git/objects/2d/\n",
            "   creating: 260CDL_22Winter/.git/objects/48/\n",
            "   creating: 260CDL_22Winter/.git/objects/84/\n",
            "   creating: 260CDL_22Winter/.git/objects/12/\n",
            "   creating: 260CDL_22Winter/.git/objects/71/\n",
            "   creating: 260CDL_22Winter/.git/objects/49/\n",
            "   creating: 260CDL_22Winter/.git/objects/25/\n",
            "  inflating: 260CDL_22Winter/.git/info/exclude  \n",
            "  inflating: 260CDL_22Winter/.git/logs/HEAD  \n",
            "   creating: 260CDL_22Winter/.git/logs/refs/\n",
            "  inflating: 260CDL_22Winter/.git/hooks/commit-msg.sample  \n",
            "  inflating: 260CDL_22Winter/.git/hooks/pre-rebase.sample  \n",
            "  inflating: 260CDL_22Winter/.git/hooks/pre-commit.sample  \n",
            "  inflating: 260CDL_22Winter/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: 260CDL_22Winter/.git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: 260CDL_22Winter/.git/hooks/pre-receive.sample  \n",
            "  inflating: 260CDL_22Winter/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: 260CDL_22Winter/.git/hooks/post-update.sample  \n",
            "  inflating: 260CDL_22Winter/.git/hooks/pre-merge-commit.sample  \n",
            "  inflating: 260CDL_22Winter/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: 260CDL_22Winter/.git/hooks/pre-push.sample  \n",
            "  inflating: 260CDL_22Winter/.git/hooks/update.sample  \n",
            "   creating: 260CDL_22Winter/.git/refs/heads/\n",
            "   creating: 260CDL_22Winter/.git/refs/tags/\n",
            "  inflating: 260CDL_22Winter/model/ranker/2.model/param.json  \n",
            "  inflating: 260CDL_22Winter/model/ranker/2.model/C.npz  \n",
            "  inflating: 260CDL_22Winter/model/ranker/2.model/W.npz  \n",
            "  inflating: 260CDL_22Winter/model/ranker/0.model/param.json  \n",
            "  inflating: 260CDL_22Winter/model/ranker/0.model/C.npz  \n",
            "  inflating: 260CDL_22Winter/model/ranker/0.model/W.npz  \n",
            "  inflating: 260CDL_22Winter/model/ranker/1.model/param.json  \n",
            "  inflating: 260CDL_22Winter/model/ranker/1.model/C.npz  \n",
            "  inflating: 260CDL_22Winter/model/ranker/1.model/W.npz  \n",
            "  inflating: 260CDL_22Winter/.git/objects/95/ec9876421a5534188d4df2c99ecbe3987eab6b  \n",
            "  inflating: 260CDL_22Winter/.git/objects/57/b3dfa71ea025cd71ebf87f968525a73357a0bc  \n",
            "  inflating: 260CDL_22Winter/.git/objects/9b/e0350ec75755ba4ab9895f55c8c286d9cff6c3  \n",
            "  inflating: 260CDL_22Winter/.git/objects/d9/0b5b3d79258b732a82ec05bccb55e8b586783f  \n",
            "  inflating: 260CDL_22Winter/.git/objects/ac/9fc3d13f609795fc5d7d22586aa3aa8addbec3  \n",
            "  inflating: 260CDL_22Winter/.git/objects/a5/73be209b41da12d6bc0ceb1eadfd82dbb06286  \n",
            "  inflating: 260CDL_22Winter/.git/objects/ab/014eaf967856dbfb3c7f6799413576d9a9cf11  \n",
            "  inflating: 260CDL_22Winter/.git/objects/c1/dc5fbdcbe18ffa0caf3da2f5a9648a50f32e83  \n",
            "  inflating: 260CDL_22Winter/.git/objects/44/06bff749c0c80867a5e3de2c99f1551a59f5cd  \n",
            "  inflating: 260CDL_22Winter/.git/objects/6b/6d7c0f6e8f9727eb6f1ea20d01b87a592781df  \n",
            "  inflating: 260CDL_22Winter/.git/objects/6e/a7ac8670f3d28ad3920ac64ce2afaafe15504d  \n",
            "  inflating: 260CDL_22Winter/.git/objects/09/5fda93c8ba7e556b419b816715a9c36df69c2d  \n",
            "  inflating: 260CDL_22Winter/.git/objects/6c/f7f9b52a3b5f0bd9aa90b105c778e6d8ecccae  \n",
            "  inflating: 260CDL_22Winter/.git/objects/ba/ae0b231f224d384c0a716725ca304cf93e3a4c  \n",
            "  inflating: 260CDL_22Winter/.git/objects/a0/0a3b0df722a60089c67b45450bb0d4844cf74c  \n",
            "  inflating: 260CDL_22Winter/.git/objects/c4/4ae981ce63051e9b85cfcda07facd9837b56fc  \n",
            "  inflating: 260CDL_22Winter/.git/objects/cb/4e3451f8aac19eb5e0b909b1901ffa6dd41d32  \n",
            "  inflating: 260CDL_22Winter/.git/objects/46/d4e3ce2e39716c40ee4358bf669921919dbb43  \n",
            "  inflating: 260CDL_22Winter/.git/objects/2c/665060c600471d73a03d9a482b2780801e1e65  \n",
            "  inflating: 260CDL_22Winter/.git/objects/2d/a0b32d40a707b23b1e8d1f6e74e10a901dc20b  \n",
            "  inflating: 260CDL_22Winter/.git/objects/48/35e485b1a1f63a86f2a7a71f1ffbc9bb72d580  \n",
            "  inflating: 260CDL_22Winter/.git/objects/84/17e77b7a739f8b1e338bf5139a7889fba2f20f  \n",
            "  inflating: 260CDL_22Winter/.git/objects/12/d1d327580e6114d29d0fe3a29c2cc53b2544c7  \n",
            "  inflating: 260CDL_22Winter/.git/objects/12/e58eae34fbf8809a6fd62880c382328f0cffd2  \n",
            "  inflating: 260CDL_22Winter/.git/objects/71/2d80d8cfbb9f15aaad437695e024da6614585e  \n",
            "  inflating: 260CDL_22Winter/.git/objects/49/f890c42136453fa0fd16f2b448109ac05fc09d  \n",
            "  inflating: 260CDL_22Winter/.git/objects/25/cdd63f06726dd2c46ff44058ab0013dc78cfbf  \n",
            "   creating: 260CDL_22Winter/.git/logs/refs/heads/\n",
            "  inflating: 260CDL_22Winter/.git/refs/heads/master  \n",
            "  inflating: 260CDL_22Winter/.git/logs/refs/heads/master  \n"
          ]
        }
      ],
      "source": [
        "! unzip /content/drive/MyDrive/CS260c/Amazon13k_Bow/AmazonCat-13K.bow.zip\n",
        "! unzip /content/drive/MyDrive/CS260c/Amazon13k_Raw/AmazonCat-13K.raw.zip\n",
        "! gunzip -k /content/AmazonCat-13K.raw/trn.json.gz\n",
        "! gunzip -k /content/AmazonCat-13K.raw/tst.json.gz\n",
        "! unzip /content/drive/MyDrive/CS260c/260CDL_22Winter.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUpyM3T2XmEJ"
      },
      "outputs": [],
      "source": [
        "from pecos.xmc.xlinear.model import XLinearModel\n",
        "from pecos.xmc import Indexer, LabelEmbeddingFactory\n",
        "from pecos.utils import smat_util\n",
        "from pecos.utils.featurization.text.preprocess import Preprocessor\n",
        "from pecos.xmc.xtransformer.model import XTransformer\n",
        "from pecos.xmc.xtransformer.module import MLProblemWithText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC9XCEZgX1x8",
        "outputId": "551c1bea-4799-43ae-9178-c6438ca0ea06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/drive/MyDrive/CS260c/train_corpus.txt': No such file or directory\n",
            "current progress:  0.08429997302400863\n",
            "current progress:  0.16859994604801726\n",
            "current progress:  0.2528999190720259\n",
            "current progress:  0.3371998920960345\n",
            "current progress:  0.42149986512004317\n",
            "current progress:  0.5057998381440518\n",
            "current progress:  0.5900998111680604\n",
            "current progress:  0.674399784192069\n",
            "current progress:  0.7586997572160777\n",
            "current progress:  0.8429997302400863\n",
            "current progress:  0.9272997032640949\n",
            "current progress:  0.9999991570002698\n",
            "(1186239, 13330)\n",
            "1186239\n"
          ]
        }
      ],
      "source": [
        "# training from raw\n",
        "# useful CSR explaination\n",
        "# https://stackoverflow.com/questions/52299420/scipy-csr-matrix-understand-indptr\n",
        "\n",
        "          # ins.  x dim  y dim\n",
        "#training 1186239 203882 13330\n",
        "#testing  306782  203882 13330\n",
        "! rm '/content/drive/MyDrive/CS260c/train_corpus.txt'\n",
        "\n",
        "corpus = []\n",
        "x_vocab = []\n",
        "\n",
        "y_indptr = [0]\n",
        "y_indice = []\n",
        "y_data = []\n",
        "y_csrs = []\n",
        "lenth = 1186239\n",
        "\n",
        "with open(\"/content/AmazonCat-13K.bow/Xf.txt\") as file:\n",
        "  x_vocab = set([line.rstrip('\\n') for line in file])\n",
        "\n",
        "with open(\"/content/AmazonCat-13K.raw/trn.json\") as myfile:\n",
        "  for i in range(1,lenth+1):\n",
        "    line = json.loads(next(myfile))\n",
        "    x_line = [itm for itm in line[\"content\"].lower().split() if itm in x_vocab]\n",
        "    corpus.append(\" \".join(x_line))\n",
        "    y_indptr.append(y_indptr[-1] + len(line[\"target_ind\"]) )\n",
        "    y_indice = y_indice + line[\"target_ind\"]\n",
        "    if i % 100000 == 0 or i == lenth:\n",
        "      print(\"current progress: \", i/(lenth+1.0) )\n",
        "      y_data = [1] * len(y_indice)\n",
        "      temp_csr = csr_matrix((y_data, y_indice, y_indptr), shape = (len(corpus),13330), dtype=np.float32 )\n",
        "      y_csrs.append(temp_csr)\n",
        "      with open('/content/drive/MyDrive/CS260c/train_corpus.txt', 'a') as f:\n",
        "        for item in corpus:\n",
        "          f.write(\"%s\\n\" % item)\n",
        "      corpus = []\n",
        "      y_indptr = [0]\n",
        "      y_indice = []\n",
        "      y_data = []\n",
        "\n",
        "y_csr = y_csrs[0]\n",
        "for item in y_csrs[1:]:\n",
        "  y_csr = scipy.sparse.vstack((y_csr,item))\n",
        "scipy.sparse.save_npz('/content/drive/MyDrive/CS260c/train_y.npz', y_csr)\n",
        "\n",
        "print(y_csr.shape)\n",
        "print(sum(1 for line in open('/content/drive/MyDrive/CS260c/train_corpus.txt')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOXJFLKMXqwc"
      },
      "outputs": [],
      "source": [
        "# loading vectorizer model \n",
        "! python3 -m pecos.utils.featurization.text.preprocess build \\\n",
        "  --text-pos 0 \\\n",
        "  --input-text-path /content/drive/MyDrive/CS260c/train_corpus.txt \\\n",
        "  --vectorizer-config-path /content/260CDL_22Winter/tfidf-model/config.json \\\n",
        "  --output-model-folder /content/tfidf-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDy_EiSGXwPj"
      },
      "outputs": [],
      "source": [
        "! python3 -m pecos.utils.featurization.text.preprocess run \\\n",
        "    --input-preprocessor-folder /content/tfidf-model \\\n",
        "    --input-text-path /content/drive/MyDrive/CS260c/train_corpus.txt \\\n",
        "    --output-inst-path /content/x_trn.npz \\\n",
        "    --text-pos 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfkdDnTnY2U3"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/CS260c/train_corpus.txt /content/train_corpus.txt\n",
        "!cp /content/drive/MyDrive/CS260c/train_y.npz /content/train_y.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQmJb7rCatxZ"
      },
      "outputs": [],
      "source": [
        "# xlm = XLinearModel.load(\"/content/260CDL_22Winter/model\", is_predict_only=False)\n",
        "xlm = XLinearModel.load(\"/content/260CDL_22Winter/model\", is_predict_only=True) # for faster prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlNUjLdba881",
        "outputId": "0b289be8-c805-4bfa-8e1d-8cc97ec3e27e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 216297)\t0.31869206\n",
            "  (0, 325375)\t0.33668044\n",
            "  (0, 358791)\t0.42867258\n",
            "  (0, 418876)\t0.169459\n",
            "  (0, 632339)\t0.20349121\n",
            "  (0, 740100)\t0.1829013\n",
            "  (0, 743608)\t0.6015959\n",
            "  (0, 822284)\t0.1499112\n",
            "  (0, 850885)\t0.1441928\n",
            "  (0, 873034)\t0.30423844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n"
          ]
        }
      ],
      "source": [
        "from pecos.utils.featurization.text.vectorizers import Vectorizer, vectorizer_dict\n",
        "vectorizer = Vectorizer.load(\"/content/260CDL_22Winter/tfidf-model\")\n",
        "\n",
        "x_csr = vectorizer.predict([\"walking to catch up to her people fleeing shoshoni she encounters\"])\n",
        "print(x_csr)\n",
        "# print(xlm.predict(x_csr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1lHSpsQeTb5",
        "outputId": "a112a50c-5fb2-42e1-8a63-ae816240bf6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 32.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 20.1 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 275 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lime) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lime) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime) (4.63.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from lime) (1.0.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime) (0.18.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (1.2.0)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (7.1.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.6.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->lime) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (3.1.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283857 sha256=d288c3a8777d42310fef8ed7c894950ce55ef6f9b375995fc66abcf9b8ba1a26\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/cb/e5/ac701e12d365a08917bf4c6171c0961bc880a8181359c66aa7\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ],
      "source": [
        "! pip install lime\n",
        "import lime\n",
        "from lime.lime_text import LimeTextExplainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15CAuqKgUooX"
      },
      "outputs": [],
      "source": [
        "# global timetime\n",
        "# timetime = 0\n",
        "# import time\n",
        "class Prob_wrapper(object):\n",
        "  def __init__(self, input_labels):\n",
        "    self.labels = input_labels\n",
        "\n",
        "  def pred_proba(self, sentence):\n",
        "    # print(sentence)\n",
        "    # s = time.time()\n",
        "    tmp = vectorizer.predict(sentence)\n",
        "    tmp.sort_indices()\n",
        "    # print(tmp)\n",
        "    y_pred = xlm.predict(tmp) \n",
        "    # global timetime\n",
        "    # timetime += (time.time() - s)\n",
        "    return y_pred[:,self.labels].toarray()\n",
        "\n",
        "# x_csr = vectorizer.predict([\"walking to catch up to her people fleeing shoshoni she encounters\"])\n",
        "# text = \"walking to catch up to her people fleeing shoshoni she encounters\"\n",
        "# prob_tool = Prob_wrapper([1471, 7083])\n",
        "# prob_tool.pred_proba([text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iNTzxg3beLu"
      },
      "outputs": [],
      "source": [
        "          # ins.  x dim  y dim\n",
        "#training 1186239 203882 13330\n",
        "#testing  306782  203882 13330\n",
        "num_instance = 1186239\n",
        "with open(\"/content/AmazonCat-13K.bow/Xf.txt\", \"r\") as f:\n",
        "  lines = f.read()\n",
        "  x_feat = lines.splitlines()\n",
        "x_feat_dict = {k: v for v, k in enumerate(x_feat)}\n",
        "# scores_all = csr_matrix((num_instance, 203882))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJqxPcYqDljI",
        "outputId": "d595080d-f3df-4230-8ab0-950a5e407039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1471 36823 0.07919903412132001\n",
            "1471 16990 0.06138835162413465\n",
            "1471 12751 0.056292187167068575\n",
            "1471 16773 0.055427279533336656\n",
            "1471 181055 0.054356260712621216\n",
            "1471 137571 0.05301980429994468\n",
            "1471 86710 0.039424443040888116\n",
            "1471 36822 0.03508712043957297\n",
            "1471 200180 -0.02991980266416263\n",
            "1471 183349 -0.023726492221345152\n",
            "\n",
            "7083 99564 -0.15467825547701655\n",
            "7083 16773 -0.1125288274947338\n",
            "7083 96591 0.0740223722952266\n",
            "7083 200401 0.06587592848187485\n",
            "7083 137571 -0.06356111277735713\n",
            "7083 195611 0.054577890120266676\n",
            "7083 16990 0.05058099248147459\n",
            "7083 12751 0.04986921098864872\n",
            "7083 191281 0.04324110768990051\n",
            "7083 12475 0.039026368835514295\n",
            "\n",
            "[{1471: {36823: 0.07919903412132001, 16990: 0.06138835162413465, 12751: 0.056292187167068575, 16773: 0.055427279533336656, 181055: 0.054356260712621216, 137571: 0.05301980429994468, 86710: 0.039424443040888116, 36822: 0.03508712043957297, 183349: -0.023726492221345152, 200180: -0.02991980266416263}, 7083: {96591: 0.0740223722952266, 200401: 0.06587592848187485, 195611: 0.054577890120266676, 16990: 0.05058099248147459, 12751: 0.04986921098864872, 191281: 0.04324110768990051, 12475: 0.039026368835514295, 137571: -0.06356111277735713, 16773: -0.1125288274947338, 99564: -0.15467825547701655}}, {1471: {36823: 0.07919903412132001, 16990: 0.06138835162413465, 12751: 0.056292187167068575, 16773: 0.055427279533336656, 181055: 0.054356260712621216, 137571: 0.05301980429994468, 86710: 0.039424443040888116, 36822: 0.03508712043957297, 183349: -0.023726492221345152, 200180: -0.02991980266416263}, 7083: {96591: 0.0740223722952266, 200401: 0.06587592848187485, 195611: 0.054577890120266676, 16990: 0.05058099248147459, 12751: 0.04986921098864872, 191281: 0.04324110768990051, 12475: 0.039026368835514295, 137571: -0.06356111277735713, 16773: -0.1125288274947338, 99564: -0.15467825547701655}}]\n"
          ]
        }
      ],
      "source": [
        "from lime.lime_text import IndexedString, TextDomainMapper\n",
        "import sklearn\n",
        "from sklearn.linear_model import Ridge\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from functools import partial\n",
        "import json\n",
        "\n",
        "# classifier_fn = Prob_wrapper([1471, 7083]).pred_proba\n",
        "class Lime_Express(object):\n",
        "  def __init__(self):\n",
        "    self.RandState = np.random.RandomState()\n",
        "\n",
        "  def feature_selection(self, data, labels, weights, num_features):\n",
        "    clf = Ridge(alpha=0.01, fit_intercept=True, random_state=self.RandState)\n",
        "    clf.fit(data, labels, sample_weight=weights)\n",
        "    coef = clf.coef_\n",
        "\n",
        "    if sp.sparse.issparse(data):\n",
        "      coef = sp.sparse.csr_matrix(clf.coef_)\n",
        "      weighted_data = coef.multiply(data[0])\n",
        "      # Note: most efficient to slice the data before reversing\n",
        "      sdata = len(weighted_data.data)\n",
        "      argsort_data = np.abs(weighted_data.data).argsort()\n",
        "      # Edge case where data is more sparse than requested number of feature importances\n",
        "      # In that case, we just pad with zero-valued features\n",
        "      if sdata < num_features:\n",
        "          nnz_indexes = argsort_data[::-1]\n",
        "          indices = weighted_data.indices[nnz_indexes]\n",
        "          num_to_pad = num_features - sdata\n",
        "          indices = np.concatenate((indices, np.zeros(num_to_pad, dtype=indices.dtype)))\n",
        "          indices_set = set(indices)\n",
        "          pad_counter = 0\n",
        "          for i in range(data.shape[1]):\n",
        "              if i not in indices_set:\n",
        "                  indices[pad_counter + sdata] = i\n",
        "                  pad_counter += 1\n",
        "                  if pad_counter >= num_to_pad:\n",
        "                      break\n",
        "      else:\n",
        "          nnz_indexes = argsort_data[sdata - num_features:sdata][::-1]\n",
        "          indices = weighted_data.indices[nnz_indexes]\n",
        "      return indices  \n",
        "    else:\n",
        "      weighted_data = coef * data[0]\n",
        "      feature_weights = sorted(\n",
        "          zip(range(data.shape[1]), weighted_data),\n",
        "          key=lambda x: np.abs(x[1]),\n",
        "          reverse=True)\n",
        "      return np.array([x[0] for x in feature_weights[:num_features]])     \n",
        "\n",
        "  def distance_fn(self, x):\n",
        "      return sklearn.metrics.pairwise.pairwise_distances(\n",
        "          x, x[0], metric=\"cosine\").ravel() * 100\n",
        "\n",
        "  def __data_labels_distances(self, indexed_string,\n",
        "                              classifier_fn,\n",
        "                              num_samples,\n",
        "                              distance_metric='cosine'):\n",
        "    doc_size = indexed_string.num_words()\n",
        "    sample = np.random.randint(1, doc_size + 1, num_samples - 1)\n",
        "    data = np.ones((num_samples, doc_size))\n",
        "    data[0] = np.ones(doc_size)\n",
        "    features_range = range(doc_size)\n",
        "    inverse_data = [indexed_string.raw_string()]\n",
        "    for i, size in enumerate(sample, start=1):\n",
        "      inactive = np.random.choice(features_range, size, replace=False)\n",
        "      data[i, inactive] = 0\n",
        "      inverse_data.append(indexed_string.inverse_removing(inactive))\n",
        "    labels = classifier_fn(inverse_data)\n",
        "    distances = self.distance_fn(sp.sparse.csr_matrix(data))\n",
        "    return data, labels, distances\n",
        "\n",
        "  # explain_instance_with_data(data, yss, distances, label, num_features)\n",
        "  def explain_instance_with_data(self, \n",
        "                                  neighborhood_data,\n",
        "                                  neighborhood_labels,\n",
        "                                  distances,\n",
        "                                  label,\n",
        "                                  num_features,):\n",
        "    \n",
        "    def kernel(d, kernel_width):\n",
        "      return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
        "\n",
        "    kernel_width = 25\n",
        "    kernel_fn = partial(kernel, kernel_width=kernel_width)\n",
        "\n",
        "    weights = kernel_fn(distances)\n",
        "    labels_column = neighborhood_labels[:, label]\n",
        "    used_features = self.feature_selection(neighborhood_data, labels_column, weights, num_features)\n",
        "    # model_regressor = Ridge(alpha=1, fit_intercept=True, random_state=np.random.RandomState())\n",
        "    easy_model = Ridge(alpha=1, fit_intercept=True, random_state=np.random.RandomState())\n",
        "    easy_model.fit(neighborhood_data[:, used_features],\n",
        "                    labels_column, sample_weight=weights)\n",
        "    return zip(used_features, easy_model.coef_)\n",
        "  \n",
        "  def explain(self, text_instance, labels, classifier_fn, num_samples, num_features):\n",
        "  # text_instance = \"people fleeing shoshoni she encounters\"\n",
        "    indexed_string = (IndexedString(text_instance, bow=True, split_expression = r'\\W+', mask_string=None))\n",
        "    domain_mapper = TextDomainMapper(indexed_string)\n",
        "    data, yss, distances = self.__data_labels_distances(indexed_string, classifier_fn, num_samples)\n",
        "    # labels = list(range(2)) #TODO modefy！！！\n",
        "    local_pred = {}\n",
        "    for label in labels:\n",
        "      local_pred[label] = self.explain_instance_with_data(data, yss, distances, label, num_features)\n",
        "    return local_pred\n",
        "      \n",
        "\n",
        "test_instance = \"set in the northern plains in the years just before the blackfeet tribe acquired this is story about an resourceful blackfeet indian spotted flower is able to elude capture and survive hunger and miles of walking to catch up to her people fleeing shoshoni she encounters riderless wounded in buffalo overcoming her fear takes the animal she calls or elk that works like to her people so they can use it to rescue her two captured girl friends accurate background information has been carefully integrated into credible and consciously piece of historical journala fictitious account of the coming of the horse to the blackfeet indians is told in spotted flower and the the story centers around blackfeet girl of the spotted who \"\n",
        "lime_exp = Lime_Express()\n",
        "explainer = LimeTextExplainer()\n",
        "test_labels = list(range(2))\n",
        "labels = [1471,7083]\n",
        "prob_tool = Prob_wrapper(labels)\n",
        "# pred_test = lime_exp.explain(test_instance, test_labels, prob_tool.pred_proba, 10000, 10)\n",
        "exp = explainer.explain_instance(test_instance, prob_tool.pred_proba, num_samples=10000, num_features=10, top_labels = len(labels))\n",
        "pred_test = exp.local_exp\n",
        "\n",
        "main_dict = {}\n",
        "for lb in test_labels:\n",
        "  lb_dict = {}\n",
        "  for itema,itemb in pred_test[lb]:\n",
        "    # print(lb, itema, itemb)\n",
        "    lb_dict[ x_feat_dict[test_instance.split()[itema]] ] = itemb\n",
        "    print(labels[lb], x_feat_dict[test_instance.split()[itema]],itemb)\n",
        "  feat_sort = dict(sorted(lb_dict.items(), key=lambda item: item[1], reverse=True) )\n",
        "  main_dict[labels[lb]] = feat_sort\n",
        "  print(\"\")\n",
        "\n",
        "ls_tt = [main_dict, main_dict]\n",
        "with open('test.jsonl', 'w') as f:\n",
        "  for ob in ls_tt:\n",
        "    json.dump(ob, f)\n",
        "    f.write('\\n')\n",
        "    # json.dump(main_dict, f)\n",
        "\n",
        "data = []\n",
        "def keystoint(x):\n",
        "    return {int(k): v for k, v in x.items()}\n",
        "with open('test.jsonl') as f:\n",
        "  for line in f:\n",
        "    data.append(json.loads(line, object_hook=keystoint))\n",
        "print(data)\n",
        "\n",
        "# (json.loads(j, object_hook=keystoint))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJKbPKpEk33h",
        "outputId": "8fcfcd47-9314-4510-bcb5-332a81f8fcc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current progress:  1.0\n"
          ]
        }
      ],
      "source": [
        "# global timetime\n",
        "# timetime = 0\n",
        "explainer = LimeTextExplainer()\n",
        "scores_all = csr_matrix((1, 203882))\n",
        "timea = 0\n",
        "timeb = 0\n",
        "lime_expr = Lime_Express()\n",
        "json_txt = \"\"\n",
        "\n",
        "# y_csr = scipy.sparse.load_npz('/content/drive/MyDrive/CS260c/train_y.npz')\n",
        "y_csr = scipy.sparse.load_npz('/content/train_y.npz')\n",
        "idx_cnter = 0\n",
        "# with open(\"/content/drive/MyDrive/CS260c/train_corpus.txt\") as f:\n",
        "with open(\"/content/train_corpus.txt\") as f:\n",
        "  idx_cnter = 1186000\n",
        "  for _ in range(idx_cnter): # to be used when break\n",
        "    next(f)\n",
        "\n",
        "  for line in f:\n",
        "    scores = np.zeros(203882)\n",
        "    txt = line\n",
        "    lbl = y_csr[idx_cnter,:].indices\n",
        "    prob_tool = Prob_wrapper(lbl)\n",
        "\n",
        "    if len(txt) > 1 :\n",
        "      # sa = time.time()\n",
        "      num_sps = min(len(txt), 100)\n",
        "      #LIME: \n",
        "      # exp = explainer.explain_instance(txt, prob_tool.pred_proba, num_samples=num_sps, num_features=100, top_labels = len(lbl))\n",
        "      #MY_LIME: \n",
        "      reind_labels = list(range(len(lbl)))\n",
        "      explain_pairs = lime_expr.explain(txt, reind_labels, prob_tool.pred_proba, num_sps, 100)\n",
        "      # ea = time.time()\n",
        "      # timea += ea - sa\n",
        "\n",
        "      # sb = time.time()\n",
        "      #LIME: \n",
        "      # for itm in exp.local_exp:\n",
        "      #   for idx, score in exp.local_exp[itm]:\n",
        "      #MY_LIME: \n",
        "      label_dict = {}\n",
        "      for itm in explain_pairs:\n",
        "        feat_dict = {}\n",
        "        for idx, score in explain_pairs[itm]:\n",
        "          f_index = x_feat_dict[txt.split()[idx]]\n",
        "          feat_dict[ str(f_index) ] = score\n",
        "        feat_sort = dict(sorted(feat_dict.items(), key=lambda item: item[1], reverse=True) )\n",
        "        label_dict[ str(lbl[itm]) ] = feat_sort\n",
        "      # eb = time.time()\n",
        "      # timeb += eb - sb\n",
        "      json_txt += json.dumps(label_dict)\n",
        "      json_txt += \"\\n\"\n",
        "    else:\n",
        "      json_txt += json.dumps({\"-1\":0})\n",
        "      json_txt += \"\\n\"\n",
        "      print(\"omitting long case: \", idx_cnter)\n",
        "\n",
        "    # scores = scipy.sparse.csr_matrix(scores)\n",
        "    # scores_all = scipy.sparse.vstack((scores_all,scores))\n",
        "\n",
        "    idx_cnter += 1\n",
        "    # if idx_cnter % 1000 == 0:\n",
        "    if idx_cnter == 1186239:\n",
        "      name = \"/content/drive/MyDrive/CS260c/Xlr_feat_scores/scores\" + str(idx_cnter) + \".jsonl\"\n",
        "      with open(name, \"a\") as f:\n",
        "        f.write(json_txt)\n",
        "      json_txt = \"\"\n",
        "      # name = '/content/drive/MyDrive/CS260c/tmp/train_impo_mtx_'+str(idx_cnter)+'.npz'\n",
        "      # scipy.sparse.save_npz(name, scores_all)\n",
        "      print(\"current progress: \", idx_cnter/num_instance)\n",
        "# print(scores_all)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95Bw0fOaTLMA",
        "outputId": "38f3860e-3c1f-477c-c2b8-04119825316c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15.568015336990356 40.097657680511475 4.675274133682251\n"
          ]
        }
      ],
      "source": [
        "# time: pred lime-expain other\n",
        "# total 78 sec \n",
        "# 30.36753273010254 75.35491013526917 1.4491801261901855\n",
        "# 30.87858748435974 76.01335167884827 1.3487579822540283\n",
        "print(timetime, timea, timeb)\n",
        "\n",
        "#lime time 400: 15.802817821502686 43.736669301986694 5.115905046463013"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFIb3dT_xEi2"
      },
      "outputs": [],
      "source": [
        "# create a new corpus with importan feature removed\n",
        "from itertools import islice\n",
        "! rm /content/removed_train_corpus.txt\n",
        "! touch /content/removed_train_corpus.txt\n",
        "\n",
        "x_feat_dict_inv = {v: k for k, v in x_feat_dict.items()}\n",
        "subfile_index = list(range(1000, 1187000, 1000)) + [1186239]\n",
        "\n",
        "with open(\"/content/train_corpus.txt\", 'r') as corpus, open(\"/content/removed_train_corpus.txt\", 'a') as new_corpus:\n",
        "  for subfile_id in subfile_index:\n",
        "    print(\"current progress: \", subfile_id / 1186239)\n",
        "    subfile_name = \"/content/drive/MyDrive/CS260c/Xlr_feat_scores/scores\"+str(subfile_id)+\".jsonl\"\n",
        "    with open(subfile_name, 'r') as json_file:\n",
        "        json_list = list(json_file)\n",
        "\n",
        "    for json_str in json_list:\n",
        "      flag = 0\n",
        "      result = json.loads(json_str)\n",
        "      important_feat = set()\n",
        "      for keys, value in result.items():\n",
        "        try:\n",
        "          n_items = islice(value.items(), 5)\n",
        "          l = [nword[1] for nword in n_items]\n",
        "          important_feat.update(l)\n",
        "          # important_feat.add( x_feat_dict_inv[ int(next(iter(value))) ] )\n",
        "        except:\n",
        "          flag = 1\n",
        "      if flag == 0:\n",
        "        stopwords = list(important_feat)\n",
        "        query_ls = next(corpus).split()\n",
        "        removed_ls  = [word for word in query_ls if word.lower() not in stopwords]\n",
        "        removed = ' '.join(removed_ls)\n",
        "        new_corpus.write(\"%s\\n\" % removed)\n",
        "      else: \n",
        "        new_corpus.write(\"%s\" % next(corpus) )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! rm /content/drive/MyDrive/CS260c/removed_train_corpus.txt\n",
        "! cp /content/removed_train_corpus.txt /content/drive/MyDrive/CS260c/removed_train_corpus.txt"
      ],
      "metadata": {
        "id": "DVXpIp0S-zIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 -m pecos.utils.featurization.text.preprocess run \\\n",
        "    --input-preprocessor-folder /content/260CDL_22Winter/tfidf-model \\\n",
        "    --input-text-path /content/drive/MyDrive/CS260c/removed_train_corpus.txt \\\n",
        "    --output-inst-path /content/removed_x_trn.npz \\\n",
        "    --text-pos 0\n",
        "\n",
        "# ! python3 -m pecos.utils.featurization.text.preprocess run \\\n",
        "#     --input-preprocessor-folder /content/260CDL_22Winter/tfidf-model \\\n",
        "#     --input-text-path /content/drive/MyDrive/CS260c/test_corpus.txt \\\n",
        "#     --output-inst-path /content/x_tst.npz \\\n",
        "#     --text-pos 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WeLMIH-_UeJ",
        "outputId": "a4561f4b-4344-49be-f074-21e131451262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = XLinearModel.load_feature_matrix(\"/content/removed_x_trn.npz\")\n",
        "Y = XLinearModel.load_label_matrix(\"/content/train_y.npz\", for_training=True)\n",
        "\n",
        "# # construct label feature for clustering\n",
        "label_feat = LabelEmbeddingFactory.create(Y, X, method=\"pifa\")\n",
        "\n",
        "# # generate label indexing\n",
        "cluster_chain = Indexer.gen(label_feat, indexer_type=\"hierarchicalkmeans\")\n",
        "\n",
        "xlm = XLinearModel.train(X, Y, C=cluster_chain)\n",
        "print(\"chain length is \", len(cluster_chain))\n",
        "\n",
        "xlm.save(\"new_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTrwIqP7_2EP",
        "outputId": "dd695a18-e522-4958-8786-a236bfc39d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chain length is  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! python3 -m pecos.utils.featurization.text.preprocess run \\\n",
        "#     --input-preprocessor-folder /content/260CDL_22Winter/tfidf-model \\\n",
        "#     --input-text-path /content/drive/MyDrive/CS260c/train_corpus.txt \\\n",
        "#     --output-inst-path /content/x_trn.npz \\\n",
        "#     --text-pos 0\n",
        "\n",
        "X = XLinearModel.load_feature_matrix(\"/content/x_trn.npz\")\n",
        "Y = XLinearModel.load_label_matrix(\"/content/train_y.npz\", for_training=True)\n",
        "\n",
        "# # construct label feature for clustering\n",
        "label_feat = LabelEmbeddingFactory.create(Y, X, method=\"pifa\")\n",
        "\n",
        "# # generate label indexing\n",
        "cluster_chain = Indexer.gen(label_feat, indexer_type=\"hierarchicalkmeans\")\n",
        "\n",
        "xlm = XLinearModel.train(X, Y, C=cluster_chain)\n",
        "print(\"chain length is \", len(cluster_chain))\n",
        "\n",
        "xlm.save(\"old_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5PecXPLo7HI",
        "outputId": "d19adbc8-1161-4daf-b200-5ff8d3558b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chain length is  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# xlm = XLinearModel.load(\"/content/260CDL_22Winter/model\", is_predict_only=True) # for faster prediction\n",
        "xlm = XLinearModel.load(\"/content/old_model\", is_predict_only=True) # for faster prediction\n",
        "xlm_new = XLinearModel.load(\"/content/new_model\", is_predict_only=True) # for faster prediction\n",
        "Xt = XLinearModel.load_feature_matrix(\"/content/x_tst.npz\")\n",
        "\n",
        "Y_pred = xlm.predict(Xt)\n",
        "Y_pred_new = xlm_new.predict(Xt)"
      ],
      "metadata": {
        "id": "iy2XtecsL0-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pecos.utils import smat_util\n",
        "# load test labels with shape=(Nt, L)\n",
        "\n",
        "Yt = XLinearModel.load_label_matrix(\"/content/drive/MyDrive/CS260c/test_y.npz\")\n",
        "metric = smat_util.Metrics.generate(Yt, Y_pred, topk=10)\n",
        "print(metric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqkvMYqCbNKK",
        "outputId": "56eee33a-2331-4c2d-99cf-e9e3405d4719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prec   = 90.25 82.55 74.69 67.43 60.20 53.70 48.35 43.93 40.23 37.11\n",
            "recall = 25.46 44.06 56.64 65.53 70.95 74.33 76.78 78.65 80.17 81.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric = smat_util.Metrics.generate(Yt, Y_pred_new, topk=10)\n",
        "print(metric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxLnK3xPec6C",
        "outputId": "74ce5172-9293-4160-b51d-65b468603794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prec   = 90.25 82.55 74.69 67.43 60.20 53.70 48.35 43.93 40.23 37.11\n",
            "recall = 25.46 44.06 56.64 65.53 70.95 74.33 76.78 78.65 80.17 81.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 -m pecos.utils.featurization.text.preprocess run \\\n",
        "    --input-preprocessor-folder /content/260CDL_22Winter/tfidf-model \\\n",
        "    --input-text-path /content/drive/MyDrive/CS260c/train_corpus.txt \\\n",
        "    --output-inst-path /content/x_trn.npz \\\n",
        "    --text-pos 0\n",
        "\n",
        "xlm = XLinearModel.load(\"/content/260CDL_22Winter/model\", is_predict_only=True) # for faster prediction\n",
        "xlm_new = XLinearModel.load(\"/content/new_model\", is_predict_only=True) # for faster prediction\n",
        "Xtr = XLinearModel.load_feature_matrix(\"/content/x_trn.npz\")\n",
        "\n",
        "Y_pred = xlm.predict(Xtr)\n",
        "Y_pred_new = xlm_new.predict(Xtr)\n",
        "\n",
        "from pecos.utils import smat_util\n",
        "# load test labels with shape=(Nt, L)\n",
        "\n",
        "Y_tr = XLinearModel.load_label_matrix(\"/content/train_y.npz\")\n",
        "metric = smat_util.Metrics.generate(Y_tr, Y_pred, topk=10)\n",
        "print(metric)\n",
        "metric = smat_util.Metrics.generate(Y_tr, Y_pred_new, topk=10)\n",
        "print(metric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV6h7ftGpQL6",
        "outputId": "08747de2-34d0-4881-f221-294fff46c807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "prec   = 89.66 83.31 76.22 69.56 62.74 56.36 50.99 46.51 42.72 39.50\n",
            "recall = 25.42 44.60 57.84 67.40 73.56 77.42 80.18 82.33 84.01 85.43\n",
            "prec   = 94.82 89.61 82.90 76.42 69.08 62.05 56.10 51.09 46.86 43.26\n",
            "recall = 27.33 48.55 63.28 74.16 80.75 84.68 87.41 89.41 90.96 92.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "          # ins.  x dim  y dim\n",
        "#training 1186239 203882 13330\n",
        "#testing  306782  203882 13330\n",
        "# ! rm /content/drive/MyDrive/CS260c/test_corpus.txt\n",
        "# ! touch /content/drive/MyDrive/CS260c/test_corpus.txt\n",
        "\n",
        "corpus = []\n",
        "x_vocab = []\n",
        "\n",
        "y_indptr = [0]\n",
        "y_indice = []\n",
        "y_data = []\n",
        "y_csrs = []\n",
        "lenth = 306782\n",
        "\n",
        "with open(\"/content/AmazonCat-13K.bow/Xf.txt\") as file:\n",
        "  x_vocab = set([line.rstrip('\\n') for line in file])\n",
        "\n",
        "with open(\"/content/AmazonCat-13K.raw/tst.json\") as myfile:\n",
        "  for i in range(1,lenth+1):\n",
        "    line = json.loads(next(myfile))\n",
        "    x_line = [itm for itm in line[\"content\"].lower().split() if itm in x_vocab]\n",
        "    corpus.append(\" \".join(x_line))\n",
        "    y_indptr.append(y_indptr[-1] + len(line[\"target_ind\"]) )\n",
        "    y_indice = y_indice + line[\"target_ind\"]\n",
        "    if i % 30000 == 0 or i == lenth:\n",
        "      print(\"current progress: \", i/(lenth+1.0) )\n",
        "      y_data = [1] * len(y_indice)\n",
        "      temp_csr = csr_matrix((y_data, y_indice, y_indptr), shape = (len(corpus),13330), dtype=np.float32 )\n",
        "      y_csrs.append(temp_csr)\n",
        "\n",
        "      # with open('/content/drive/MyDrive/CS260c/test_corpus.txt', 'a') as f:\n",
        "      #   print()\n",
        "        # for item in corpus:\n",
        "        #   print(item)\n",
        "        #   f.write(\"%s\\n\" % item)\n",
        "\n",
        "      corpus = []\n",
        "      y_indptr = [0]\n",
        "      y_indice = []\n",
        "      y_data = []\n",
        "\n",
        "y_csr = y_csrs[0]\n",
        "for item in y_csrs[1:]:\n",
        "  y_csr = scipy.sparse.vstack((y_csr,item))\n",
        "scipy.sparse.save_npz('/content/drive/MyDrive/CS260c/test_y.npz', y_csr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5YOIpYWMoYZ",
        "outputId": "18f6f6ba-6da2-40de-dc5e-06348d6860df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current progress:  0.09778899091540275\n",
            "current progress:  0.1955779818308055\n",
            "current progress:  0.2933669727462082\n",
            "current progress:  0.391155963661611\n",
            "current progress:  0.4889449545770137\n",
            "current progress:  0.5867339454924164\n",
            "current progress:  0.6845229364078192\n",
            "current progress:  0.782311927323222\n",
            "current progress:  0.8801009182386247\n",
            "current progress:  0.9778899091540274\n",
            "current progress:  0.9999967403669695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7jdQ6tMp9go"
      },
      "source": [
        "## Some Random Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GeQdyyYIqvA",
        "outputId": "fc2abaa7-a2ea-4c0b-c6aa-2bfd8b05c0a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.40306005"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 6295 - 1447 diff: 2.2548818588263853e-05\n",
        "# 0.4030375\n",
        "wd = W[:,6295].toarray().squeeze()\n",
        "np.dot(xd, wd) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RRf5T-4tgZu",
        "outputId": "9c3d4245-f71f-4ecc-8e24-ee72b66a49ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1,  3,  7,  6,  3,  5,  2, 10,  5,  4])"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#some random test\n",
        "doc_size = 10\n",
        "num_samples = 11\n",
        "np.random.randint(1, doc_size + 1, num_samples - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk2jzUUAZ_8U"
      },
      "outputs": [],
      "source": [
        "for itm in exp.local_exp:\n",
        "  for idx, score in exp.local_exp[itm]:\n",
        "    f_importance = score if score > 0 else 0\n",
        "    f_index = x_feat_dict[txt.split()[idx]]\n",
        "    # print(f_index, f_importance) # index of the feature in Xf.txt, score\n",
        "    scores_all[TODO_index_instance,f_index] += f_importance\n",
        "\n",
        "print(scores_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m2xbQquUQJ_",
        "outputId": "78dac4af-3f6a-457a-e613-fdcd6b594e35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: [(2, -0.1850065399617813),\n",
              "  (8, 0.177850567734265),\n",
              "  (4, 0.11475629267979169),\n",
              "  (1, 0.09533239002330907),\n",
              "  (3, -0.09343291439751804),\n",
              "  (0, 0.08113047075504802),\n",
              "  (7, -0.05788686203560823),\n",
              "  (6, 0.022232607830375374),\n",
              "  (5, -0.01789452333922869),\n",
              "  (9, 0.00944180832892001)],\n",
              " 1: [(7, 0.08482325810074662),\n",
              "  (4, 0.07322527739956748),\n",
              "  (9, 0.06711346179032968),\n",
              "  (6, 0.06710767390087848),\n",
              "  (2, 0.04532743386283294),\n",
              "  (3, -0.03916997918066874),\n",
              "  (1, 0.022340249353170173),\n",
              "  (8, 0.013145045375610365),\n",
              "  (5, -0.0021419960452150116),\n",
              "  (0, 0.0005242566498578924)],\n",
              " 2: [(4, 0.18718297533730477),\n",
              "  (7, 0.11592205600213254),\n",
              "  (0, 0.0968107009115648),\n",
              "  (8, 0.07767386226658486),\n",
              "  (5, -0.0661630002760682),\n",
              "  (9, -0.06235631322843232),\n",
              "  (1, -0.057810792500153495),\n",
              "  (2, -0.02757597446894796),\n",
              "  (6, -0.027100377832954367),\n",
              "  (3, 0.00011764991871369418)],\n",
              " 3: [(6, 0.1683065213042788),\n",
              "  (9, 0.1429017794016408),\n",
              "  (0, 0.09609811369680278),\n",
              "  (2, -0.08857669253933882),\n",
              "  (4, -0.08156220140375506),\n",
              "  (3, -0.037211475521835825),\n",
              "  (1, 0.03525197625415099),\n",
              "  (5, -0.026657022193864555),\n",
              "  (8, -0.007165393731475516),\n",
              "  (7, 0.0024558648347045397)]}"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for itm,lbl in zip(test_txt,text_lbl):\n",
        "txt = \"walking to catch up to her people fleeing shoshoni she encounters\"\n",
        "lbl = [1471,7083,13199,4038]\n",
        "prob_tool = Prob_wrapper(lbl)\n",
        "explainer = LimeTextExplainer()\n",
        "exp = explainer.explain_instance(txt, prob_tool.pred_proba, num_samples=200, num_features=1000, top_labels = len(lbl))\n",
        "exp.local_exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9U1GKL4pBvX",
        "outputId": "34080028-5a55-419b-8897-d669424575ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.4030375  0.29164279 0.29138184]\n",
            " [0.3944682  0.20462596 0.4537155 ]]\n"
          ]
        }
      ],
      "source": [
        "true_labels = [1471,7083, 13199]\n",
        "def pred_proba(sentence):\n",
        "  if type(sentence) == str:\n",
        "    sentence = [sentence]\n",
        "  # print(sentence)\n",
        "  tmp = vectorizer.predict(sentence)\n",
        "  tmp.sort_indices()\n",
        "  y_pred = xlm.predict(tmp) \n",
        "  # print(y_pred[:,true_labels].todense())\n",
        "  return y_pred[:,true_labels].toarray()\n",
        "\n",
        "sentence = [\"walking to catch up to her people fleeing shoshoni she encounters\"]\n",
        "sentence_mod = [\"walking to catch up to her people fleeing shoshoni she\"]\n",
        "corpus = sentence + sentence_mod\n",
        "print(pred_proba(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LickUZNhb2jM"
      },
      "outputs": [],
      "source": [
        "explainer = LimeTextExplainer()\n",
        "# text = \"cherry she 1990 release english at just before\"\n",
        "text = \"walking to catch up to her people fleeing shoshoni she encounters\"\n",
        "# exp = explainer.explain_instance(text, pred_proba, num_samples=200, num_features=12)\n",
        "exp = explainer.explain_instance(text, pred_proba, num_samples=200, num_features=10000, top_labels = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzrKHCimjkYy",
        "outputId": "1bc88459-27b5-49ad-fbeb-c4c41075916a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: [(2, -0.19057729959912303),\n",
              "  (8, 0.18785342663371243),\n",
              "  (3, -0.1067469536264798),\n",
              "  (4, 0.08911653805771073),\n",
              "  (1, 0.08857007928651663),\n",
              "  (0, 0.06641768957006164),\n",
              "  (7, -0.0364956649974962),\n",
              "  (5, -0.013020880804189745),\n",
              "  (6, 0.005116026707945398),\n",
              "  (9, 0.002091465596505653)],\n",
              " 1: [(7, 0.08369485719086744),\n",
              "  (6, 0.06288299324720106),\n",
              "  (9, 0.05785127863821753),\n",
              "  (4, 0.057320196487319355),\n",
              "  (3, -0.04264028479271502),\n",
              "  (2, 0.035640226547553805),\n",
              "  (1, 0.017842465564241523),\n",
              "  (5, 0.0128108430306342),\n",
              "  (8, 0.010687321222354381),\n",
              "  (0, 0.006541419335620768)],\n",
              " 2: [(7, 0.14746150316519316),\n",
              "  (4, 0.1453337592616695),\n",
              "  (9, -0.09159898217894111),\n",
              "  (8, 0.08710139866535505),\n",
              "  (0, 0.08091909986386764),\n",
              "  (2, -0.07340787599382086),\n",
              "  (6, -0.0380752978942441),\n",
              "  (1, -0.027243640264635497),\n",
              "  (5, -0.010635117047637891),\n",
              "  (3, 0.001000385863619437)]}"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "exp.local_exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQLW2jG6bzkk",
        "outputId": "ab129a0c-4445-446d-f491-b13dbeda8911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "def pred_proba(sentence):\n",
        "  if type(sentence) == str:\n",
        "    sentence = [sentence]\n",
        "  # print(sentence)\n",
        "  tmp = vectorizer.predict(sentence)\n",
        "  tmp.sort_indices()\n",
        "  y_pred = xlm.predict(tmp) \n",
        "  return y_pred.todense()\n",
        "\n",
        "sentence = [\"walking to catch up to her people fleeing shoshoni she encounters\"]\n",
        "sentence_mod = [\"walking to catch up to her people fleeing shoshoni she\"]\n",
        "corpus = sentence + sentence_mod\n",
        "print(pred_proba(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHjB-n0mLmK8",
        "outputId": "a0e58601-f5e1-4a0a-8429-71fe7c683cff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([2, 9, 6, 0, 5])"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# customize my explain function to reduce time\n",
        "from sklearn.linear_model import Ridge\n",
        "def feature_selection(data, labels, weights, num_features = 5):\n",
        "  print(1)\n",
        "  clf = Ridge(alpha=0.01, fit_intercept=True, random_state=np.random.RandomState())\n",
        "  clf.fit(data, labels, sample_weight=weights)\n",
        "  coef = clf.coef_\n",
        "\n",
        "  if sp.sparse.issparse(data):\n",
        "      coef = sp.sparse.csr_matrix(clf.coef_)\n",
        "      weighted_data = coef.multiply(data[0])\n",
        "      # Note: most efficient to slice the data before reversing\n",
        "      sdata = len(weighted_data.data)\n",
        "      argsort_data = np.abs(weighted_data.data).argsort()\n",
        "      # Edge case where data is more sparse than requested number of feature importances\n",
        "      # In that case, we just pad with zero-valued features\n",
        "      if sdata < num_features:\n",
        "          nnz_indexes = argsort_data[::-1]\n",
        "          indices = weighted_data.indices[nnz_indexes]\n",
        "          num_to_pad = num_features - sdata\n",
        "          indices = np.concatenate((indices, np.zeros(num_to_pad, dtype=indices.dtype)))\n",
        "          indices_set = set(indices)\n",
        "          pad_counter = 0\n",
        "          for i in range(data.shape[1]):\n",
        "              if i not in indices_set:\n",
        "                  indices[pad_counter + sdata] = i\n",
        "                  pad_counter += 1\n",
        "                  if pad_counter >= num_to_pad:\n",
        "                      break\n",
        "      else:\n",
        "          nnz_indexes = argsort_data[sdata - num_features:sdata][::-1]\n",
        "          indices = weighted_data.indices[nnz_indexes]\n",
        "      return indices\n",
        "  else:\n",
        "      weighted_data = coef * data[0]\n",
        "      feature_weights = sorted(\n",
        "          zip(range(data.shape[1]), weighted_data),\n",
        "          key=lambda x: np.abs(x[1]),\n",
        "          reverse=True)\n",
        "      return np.array([x[0] for x in feature_weights[:num_features]])    \n",
        "\n",
        "neighborhood_data =  np.array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        " [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        " [1., 1., 0., 0., 0., 1., 1., 1., 1., 0.],\n",
        " [0., 1., 1., 0., 1. ,0., 0., 0., 1., 1.],\n",
        " [1., 1., 1., 0., 0., 1., 0., 1., 1., 1.]])\n",
        "labels_column = [0.4030375,  0.06970668, 0.5081317,  0.501127,   0.31851748]\n",
        "weights = [1.00000000e+00, 3.35462628e-04, 6.66008203e-01, 5.03439617e-01,8.07800970e-01] \n",
        "feature_selection(neighborhood_data, labels_column, weights)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "ls = ['a','b','c','d','e','f','g','h']\n",
        "\n",
        "to_delete = set(random.sample(range(len(ls)),3))\n",
        "[x for i,x in enumerate(ls) if not i in to_delete]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lym-7bhgzphn",
        "outputId": "24699fdd-e661-41ad-bcd5-04cb1e50e423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['b', 'd', 'e', 'f', 'h']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXzTtYvjonRt",
        "outputId": "93bb5c43-ccef-4d2c-eae0-d45ac9f0c7ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['walking to catch up to her people fleeing shoshoni she encounters', '  catch up   people   she encounters', '  catch   her people fleeing shoshoni she ', ' to catch up to      encounters', ' to catch  to her   shoshoni  ', '  catch up   people fleeing  she ', '   up   people   she encounters', ' to catch  to  people fleeing   encounters', '          ', 'walking to catch up to her  fleeing shoshoni she encounters', 'walking to catch up to  people fleeing shoshoni she ', ' to catch  to her people  shoshoni  encounters', '  catch   her  fleeing   encounters', 'walking        shoshoni  ', 'walking to   to her people fleeing shoshoni she encounters', ' to   to her    she ', '          ', ' to   to her   shoshoni  ', ' to  up to  people fleeing   ', '   up      she encounters', 'walking          ', '       fleeing   encounters', '          ', '  catch up  her people   she encounters', '          ', '   up  her   shoshoni  ', '          ', 'walking   up       ', ' to   to   fleeing   encounters', ' to   to   fleeing shoshoni  ', '     her     ', 'walking to catch  to her people fleeing shoshoni she encounters', 'walking  catch   her  fleeing   ', 'walking   up  her  fleeing  she encounters', 'walking to   to her people    encounters', 'walking to  up to    shoshoni  encounters', ' to catch up to her people fleeing  she encounters', ' to  up to  people fleeing   encounters', 'walking  catch up  her people fleeing shoshoni she encounters', 'walking      people    encounters', 'walking to catch up to her people  shoshoni  ', 'walking to   to her    she ', '       fleeing   encounters', '          ', 'walking  catch      shoshoni  ', '     her  fleeing  she encounters', '     her     ', 'walking to  up to   fleeing shoshoni she ', ' to catch up to    shoshoni  ', 'walking   up  her people   she encounters', 'walking       fleeing  she encounters', '   up   people fleeing shoshoni  ', '          ', 'walking to   to her people fleeing shoshoni she ', 'walking  catch up  her  fleeing  she ', '          ', 'walking      people  shoshoni she encounters', 'walking to catch  to her   shoshoni she encounters', 'walking to catch up to her people  shoshoni she ', 'walking  catch    people    ', ' to  up to her  fleeing   ', '          ', '         she ', '  catch   her   shoshoni  encounters', '  catch      shoshoni  ']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{0: [(2, -0.17390160855285747),\n",
              "  (8, 0.1695847553736939),\n",
              "  (1, 0.1148756549316466),\n",
              "  (4, 0.10278103020446847),\n",
              "  (3, -0.10153039659923374),\n",
              "  (0, 0.08973654364769076),\n",
              "  (7, -0.07244112964211905),\n",
              "  (9, 0.010123238725934356),\n",
              "  (6, 0.010097958204189036),\n",
              "  (5, -0.008909512009377946)]}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#some random test\n",
        "txt = \"walking to catch up to her people fleeing shoshoni she encounters\"\n",
        "prob_tool = Prob_wrapper([1471])\n",
        "from lime.lime_text import IndexedString\n",
        "ts = IndexedString(txt)\n",
        "ts.num_words()\n",
        "ts.raw_string()\n",
        "\n",
        "exp = explainer.explain_instance(txt, prob_tool.pred_proba, num_samples=len(txt), num_features=20, top_labels = len(lbl))\n",
        "exp.local_exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA-UBtza3mgD",
        "outputId": "2a3a9bc1-fd87-4bb6-d54b-1ef2989bf25e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 910300)\n",
            "  (0, 1471)\t0.4030375\n",
            "  (0, 7083)\t0.29164279\n",
            "  (0, 13199)\t0.29138184\n",
            "  (0, 4038)\t0.26829728\n",
            "  (0, 11235)\t0.18322547\n",
            "  (0, 7892)\t0.17281087\n",
            "  (0, 8108)\t0.06291683\n",
            "  (0, 12630)\t0.06291683\n",
            "  (0, 12851)\t0.035093043\n",
            "  (0, 5879)\t0.034503333\n",
            "  (0, 1274)\t0.030859185\n",
            "  (0, 719)\t0.028692868\n",
            "  (0, 10185)\t0.018604755\n",
            "  (0, 4921)\t0.012175326\n",
            "  (0, 7891)\t0.010081249\n",
            "  (0, 4923)\t0.009986753\n",
            "  (0, 12583)\t0.008487523\n",
            "  (0, 12341)\t0.0075502424\n",
            "  (0, 12505)\t0.007487659\n",
            "  (0, 7719)\t0.007457245\n",
            "done 16\n",
            "done 256\n",
            "done 13330\n",
            "0.008126493310465832\n",
            "5955\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "# some random test\n",
        "import scipy\n",
        "import numpy as np\n",
        "W1 = scipy.sparse.load_npz(\"/content/260CDL_22Winter/model/ranker/0.model/W.npz\")\n",
        "W2 = scipy.sparse.load_npz(\"/content/260CDL_22Winter/model/ranker/1.model/W.npz\")\n",
        "W3 = scipy.sparse.load_npz(\"/content/260CDL_22Winter/model/ranker/2.model/W.npz\")\n",
        "# print(W[:,1471].shape)\n",
        "x_csr = vectorizer.predict([\"walking to catch up to her people fleeing shoshoni she encounters\"])\n",
        "print(x_csr.shape)\n",
        "print(xlm.predict(x_csr))\n",
        "# print(type(x_csr), type(W[:,1471]))\n",
        "\n",
        "xd = x_csr.toarray().squeeze()\n",
        "xd = np.append(xd, 1.0)\n",
        "# xd = np.insert(xd, 0, 1.0)\n",
        "base = 100\n",
        "idx = -1\n",
        "wcnt = 0\n",
        "for W, rg in zip([W1, W2, W3],[16, 256, 13330]):\n",
        "  for i in range(0, rg):\n",
        "    wd = W[:,i].toarray().squeeze()\n",
        "    rst = abs(np.dot(xd, wd) - 0.29164279)\n",
        "    if rst  < base:\n",
        "      base = rst\n",
        "      idx = i\n",
        "  print(\"done\", rg)\n",
        "  wcnt += 1\n",
        "print(base)\n",
        "print(idx)\n",
        "print(wcnt)\n",
        "\n",
        "# C = scipy.sparse.load_npz(\"/content/260CDL_22Winter/model/ranker/2.model/C.npz\")\n",
        "# C\n",
        "# 0.0002913217045769634"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "CS260c_multiLabel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}